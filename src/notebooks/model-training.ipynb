{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "module = os.path.abspath('/home/learner/wesad_experiments/src/main')\n",
    "# module = os.path.abspath(\"C:/Users\\\\18145\\\\development\\\\wesad_experiments\\\\src\\\\main\")\n",
    "if module not in sys.path:\n",
    "    sys.path.append(module)\n",
    "from DataManager import DataManager\n",
    "from Demo import Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will go ahead and do the work to prepare the data for NN creation and actually create the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for S2\n",
      "Loading data for S3\n",
      "Loading data for S4\n",
      "Loading data for S5\n",
      "Loading data for S6\n",
      "Loading data for S7\n",
      "Loading data for S8\n",
      "Loading data for S9\n",
      "Loading data for S10\n",
      "Loading data for S11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "manager.load_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features..\n",
      "\tsubject: 2\n",
      "\tsubject: 3\n",
      "\tsubject: 4\n",
      "\tsubject: 5\n",
      "\tsubject: 6\n",
      "\tsubject: 7\n",
      "\tsubject: 8\n",
      "\tsubject: 9\n",
      "\tsubject: 10\n",
      "\tsubject: 11\n",
      "conputing features..\n",
      "\tsubject: 2\n",
      "\tsubject: 3\n",
      "\tsubject: 4\n",
      "\tsubject: 5\n",
      "\tsubject: 6\n",
      "\tsubject: 7\n",
      "\tsubject: 8\n",
      "\tsubject: 9\n",
      "\tsubject: 10\n",
      "\tsubject: 11\n"
     ]
    }
   ],
   "source": [
    "manager.compute_features();\n",
    "manager.compute_features_stress();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10  subjects\n",
      "there are  44461  values for  a_mean\n",
      "there are  44461  values for  a_std\n",
      "there are  44461  values for  a_maxx\n",
      "there are  44461  values for  a_maxy\n",
      "there are  44461  values for  a_maxz\n",
      "there are  44461  values for  e_max\n",
      "there are  44461  values for  e_min\n",
      "there are  44461  values for  e_mean\n",
      "there are  44461  values for  e_range\n",
      "there are  44461  values for  e_std\n",
      "there are  44461  values for  t_max\n",
      "there are  44461  values for  t_min\n",
      "there are  44461  values for  t_mean\n",
      "there are  44461  values for  t_range\n",
      "there are  44461  values for  t_std\n"
     ]
    }
   ],
   "source": [
    "print(\"We have\", len(manager.SUBJECTS), \" subjects\")\n",
    "\n",
    "for feature in manager.FEATURES.keys():\n",
    "    print(\"there are \", len(manager.FEATURES[feature]), \" values for \", feature)\n",
    "#     print(manager.FEATURES[feature][3277:3288])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape the data such that there are [samples, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44461, 15)\n",
      "(23780, 15)\n"
     ]
    }
   ],
   "source": [
    "# Lets go ahead and reshape this data such that we have\n",
    "# samples, features = [N, 15]\n",
    "\n",
    "X1 = []\n",
    "X2 = []\n",
    "for i in range(0,  len(manager.FEATURES['a_mean'])):\n",
    "    X1.append([manager.FEATURES['a_mean'][i], manager.FEATURES['a_std'][i], manager.FEATURES['a_maxx'][i], manager.FEATURES['a_maxy'][i], manager.FEATURES['a_maxz'][i],\\\n",
    "                  manager.FEATURES['e_max'][i], manager.FEATURES['e_min'][i], manager.FEATURES['e_mean'][i], manager.FEATURES['e_range'][i], manager.FEATURES['e_std'][i],\\\n",
    "                  manager.FEATURES['t_max'][i], manager.FEATURES['t_min'][i], manager.FEATURES['t_mean'][i], manager.FEATURES['t_range'][i], manager.FEATURES['t_std'][i]])\n",
    "print(np.shape(X1))\n",
    "\n",
    "for i in range(0,  len(manager.STRESS_FEATURES['a_mean'])):\n",
    "    X2.append([manager.STRESS_FEATURES['a_mean'][i], manager.STRESS_FEATURES['a_std'][i], manager.STRESS_FEATURES['a_maxx'][i], manager.STRESS_FEATURES['a_maxy'][i], manager.STRESS_FEATURES['a_maxz'][i],\\\n",
    "                  manager.STRESS_FEATURES['e_max'][i], manager.STRESS_FEATURES['e_min'][i], manager.STRESS_FEATURES['e_mean'][i], manager.STRESS_FEATURES['e_range'][i], manager.STRESS_FEATURES['e_std'][i],\\\n",
    "                  manager.STRESS_FEATURES['t_max'][i], manager.STRESS_FEATURES['t_min'][i], manager.STRESS_FEATURES['t_mean'][i], manager.STRESS_FEATURES['t_range'][i], manager.STRESS_FEATURES['t_std'][i]] )\n",
    "print(np.shape(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(manager.FEATURES['a_mean'][100:110])\n",
    "\n",
    "# print(manager.FEATURES['a_std'][400:410])\n",
    "\n",
    "# print(manager.FEATURES['a_maxx'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0713 11:25:24.923012 12608 deprecation_wrapper.py:119] From C:\\Users\\18145\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0713 11:25:24.925736 12608 deprecation_wrapper.py:119] From C:\\Users\\18145\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0713 11:25:24.928210 12608 deprecation_wrapper.py:119] From C:\\Users\\18145\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0713 11:25:24.974790 12608 deprecation_wrapper.py:119] From C:\\Users\\18145\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=8, inter_op_parallelism_threads=8, )\n",
    "\n",
    "# tf.set_random_seed(1)\n",
    "\n",
    "# session = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize two arrays for the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 is the baseline data\n",
    "# x2 is the stress data\n",
    "\n",
    "y1 = [0] * len(X1)\n",
    "y2 = [1] * len(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44461\n",
      "23780\n"
     ]
    }
   ],
   "source": [
    "print(len(y1))\n",
    "print(len(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to concat the data between baseline and stress so that we can split it into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68241, 15)\n",
      "(68241,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X1, X2), axis=0)\n",
    "print(np.shape(X))\n",
    "\n",
    "y = np.concatenate((y1,y2), axis=0)\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data up in train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# since we don't have the same number of samples for x and y, \n",
    "# we will drop off some x values for creating the sample\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51180, 15)\n",
      "(17061, 15)\n",
      "(51180,)\n",
      "(17061,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to scale the data and hope we get better results.\n",
    "1. fit the scaler to the training data (fit_transform)\n",
    "2. transform training data with the scaler\n",
    "3. fit the model with transformed data\n",
    "4. transform test data with the scaler\n",
    "5. predict using model and output of (4)\n",
    "\n",
    "It will be better to have our data scaled between 0 and 1, so lets go ahead and create a function to \n",
    "normalize the data that way now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(data)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be\n",
    "input_shape=(number of sequences=?, time_steps=None, features=15)\n",
    "target=(number of sequences, time_steps, targets)\n",
    "\n",
    "We will use time_steps = 1 since our data has implicit timesteps from the feature calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51180, 1, 15)\n",
      "(17061, 1, 15)\n",
      "(51180,)\n",
      "(17061,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 15\n",
    "num_features = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0713 11:25:25.462407 12608 deprecation_wrapper.py:119] From C:\\Users\\18145\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the LSTM network...\n"
     ]
    }
   ],
   "source": [
    "print('Building the LSTM network...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=True))\n",
    "# model.add(LSTM(16, input_shape=(1, 15), dropout=0.35, recurrent_dropout=0.35, return_sequences=True))\n",
    "# Note:\n",
    "# Need to do return_sequences=False for the layer *before* dense\n",
    "# https://stackoverflow.com/questions/51763983/error-when-checking-target-expected-dense-1-to-have-3-dimensions-but-got-array\n",
    "model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 15)             1860      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 15)                1860      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 3,736\n",
      "Trainable params: 3,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "inputs:  (None, 1, 15)\n",
      "outputs:  (None, 1)\n",
      "actual inputs:  (51180, 1, 15)\n",
      "actual outputs:  (51180,)\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "print(\"inputs: \" , model.input_shape)\n",
    "print(\"outputs: \", model.output_shape)\n",
    "print(\"actual inputs: \", np.shape(X_train))\n",
    "print(\"actual outputs: \", np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0713 11:25:26.270827 12608 deprecation_wrapper.py:119] From C:\\Users\\18145\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0713 11:25:26.286463 12608 deprecation.py:323] From C:\\Users\\18145\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.05)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM...\n",
      "Train on 51180 samples, validate on 17061 samples\n",
      "Epoch 1/1\n",
      "51180/51180 [==============================] - 117s 2ms/step - loss: 0.1214 - acc: 0.9523 - val_loss: 0.0710 - val_acc: 0.9741\n",
      "17061/17061 [==============================] - 10s 607us/step\n"
     ]
    }
   ],
   "source": [
    "print('Training LSTM...')\n",
    "\n",
    "batch_size = 2 # I think I want to use batch_size = 1\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.07099998079985742\n",
      "accuracy: 0.9740929605533087\n"
     ]
    }
   ],
   "source": [
    "print('score:', score)\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disc\n"
     ]
    }
   ],
   "source": [
    "# Let's save the model to disc\n",
    "json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as file:\n",
    "    file.write(json)\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# Load the model of interest\n",
    "json_file = open('model.json', 'r')\n",
    "json = json_file.read()\n",
    "json_file.close()\n",
    "model_from_disc = model_from_json(json)\n",
    "model_from_disc.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17061/17061 [==============================] - 11s 644us/step\n",
      "score: 0.07099998079985742\n",
      "accuracy: 0.9740929605533087\n"
     ]
    }
   ],
   "source": [
    "opt = SGD(lr=0.05)\n",
    "model_from_disc.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "score, acc = model_from_disc.evaluate(X_test , y_test, batch_size=batch_size)\n",
    "print('score:', score)\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17061/17061 [==============================] - 10s 581us/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model_from_disc.predict(X_test, batch_size=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3851888e-05]\n",
      " [8.7159604e-01]\n",
      " [9.0369365e-05]\n",
      " ...\n",
      " [3.1206284e-03]\n",
      " [5.3005978e-02]\n",
      " [9.9578530e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     11066\n",
      "           1       0.96      0.96      0.96      5995\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     17061\n",
      "   macro avg       0.97      0.97      0.97     17061\n",
      "weighted avg       0.97      0.97      0.97     17061\n",
      "\n",
      "[[10842   224]\n",
      " [  218  5777]]\n"
     ]
    }
   ],
   "source": [
    "y_pred[y_pred>0.5] = 1 \n",
    "y_pred[y_pred<=0.5] = 0 \n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance using the LSTM based network architecture with one hidden layer\n",
    "has performed with an accuracy of ~ 97.7% on the validation set using 5 epochs.\n",
    "\n",
    "With just one epoch, the model has results between ~80% and ~92% for accuracy\n",
    "on the validation data. Each epoch of training takes approximately 70 seconds\n",
    "without GPU acceleration. At 5 epochs, the model outperforms the WESAD quoated\n",
    "results for both accuracy and F1 using less modalities and less features."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
